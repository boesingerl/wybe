{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of this notebook\n",
    "\n",
    "This Notebook is the one where we create and save all our stratified samples for the istdaten dataset.\n",
    "\n",
    "You may want to run the two stratifications on different clusters (run different sessions) because both may not fit in memory (lot of reshuffling unfortunately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "# set the application name as \"<your_gaspar_id>-homework3\"\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", cell='{ \"name\":\"%s-finalproject3\", \"executorMemory\":\"6G\", \"executorCores\":4, \"numExecutors\":10, \"driverMemory\": \"4G\" }' % username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are using Spark %s' % spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile('hdfs:///user/boesinge/finalproject/data_utils.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import data_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from scipy.stats import gamma, zscore\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from pandas.compat import StringIO\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Istdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "istdaten_df = sqlContext.read.format(\"orc\").load('/data/sbb/orc/istdaten')\n",
    "\n",
    "# by looking at the arrival and departure values we deduce the following formats\n",
    "scheduled_format = \"dd.MM.yyyy HH:mm\"\n",
    "actual_format   = \"dd.MM.yyyy HH:mm:ss\"\n",
    "\n",
    "istdaten_df = (istdaten_df\n",
    "    .withColumnRenamed('BETRIEBSTAG'        , 'trip_date')\n",
    "    .withColumnRenamed('FAHRT_BEZEICHNER'   , 'trip_id')\n",
    "    .withColumnRenamed('BETREIBER_ABK'      , 'operator_smallname')\n",
    "    .withColumnRenamed('BETREIBER_NAME'     , 'operator_name')\n",
    "    .withColumnRenamed('PRODUKT_ID'         , 'transport_type')\n",
    "    .withColumnRenamed('LINIEN_ID'          , 'line_id')\n",
    "    .withColumnRenamed('LINIEN_TEXT'        , 'line_text')\n",
    "    .withColumnRenamed('VERKEHRSMITTEL_TEXT', 'service_type')\n",
    "    .withColumnRenamed('ZUSATZFAHRT_TF'     , 'additional_trip')\n",
    "    .withColumnRenamed('FAELLT_AUS_TF'      , 'trip_failed')\n",
    "    .withColumnRenamed('HALTESTELLEN_NAME'  , 'stop_name')\n",
    "    .withColumnRenamed('ANKUNFTSZEIT'       , 'scheduled_arrival_time')\n",
    "    .withColumnRenamed('AN_PROGNOSE'        , 'actual_arrival_time')\n",
    "    .withColumnRenamed('AN_PROGNOSE_STATUS' , 'actual_arrtime_measured')\n",
    "    .withColumnRenamed('ABFAHRTSZEIT'       , 'scheduled_departure_time')\n",
    "    .withColumnRenamed('AB_PROGNOSE'        , 'actual_departure_time')\n",
    "    .withColumnRenamed('AB_PROGNOSE_STATUS' , 'actual_deptime_measured')\n",
    "    .withColumnRenamed('DURCHFAHRT_TF'      , 'not_stopping_here')\n",
    "    .withColumnRenamed('BPUIC'              , 'stop_id')\n",
    "    .withColumnRenamed('BETREIBER_ID'       , 'operator_id')\n",
    "    .withColumnRenamed('UMLAUF_ID'          , 'circuit_id')\n",
    "    .withColumn(\"actual_arrival_time\",F.unix_timestamp('actual_arrival_time', actual_format))\\\n",
    "    .withColumn(\"scheduled_arrival_time\",F.unix_timestamp('scheduled_arrival_time', scheduled_format))\\\n",
    "    .withColumn(\"actual_departure_time\",F.unix_timestamp('actual_departure_time', actual_format))\\\n",
    "    .withColumn(\"scheduled_departure_time\",F.unix_timestamp('scheduled_departure_time', scheduled_format))\n",
    "    .filter(F.col('additional_trip') == False)\n",
    "    .filter(F.col('trip_failed') == False)\n",
    "    .filter(F.col('actual_deptime_measured').isin(['REAL', 'GESCHAETZT']))\n",
    "    .filter(F.col('actual_arrtime_measured').isin(['REAL', 'GESCHAETZT']))\n",
    "    .filter(F.col('service_type').isin(data_utils.istdaten_to_groups.keys()))                              # Keep only transport types which are useful to us\n",
    "    .filter(F.dayofweek(F.from_unixtime(\"scheduled_departure_time\")).isin(['2','3','4','5','6'])) # Filter only weekdays\n",
    "    .filter(F.hour(F.from_unixtime(\"scheduled_arrival_time\"))<=24)                                # Keep only schedules in a single day\n",
    "    .filter(~(F.col('transport_type').isNull()))                                                  # Keep only nonnull service and transport types\n",
    "    .filter(~(F.col('service_type').isNull()))\n",
    "    .withColumn('transport_group', data_utils.istdaten_group('service_type'))\n",
    "    .withColumn('stop_id', data_utils.normalize_id('stop_id'))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling some data from each transport category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "istdaten_sampled = data_utils.strat_sample(istdaten_df, 'transport_group', 100000)\n",
    "istdaten_sampled.write.save(\"/user/boesinge/finalproject/istdaten_transport_group.parquet\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute arrival departure edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays = data_utils.compute_delays(data_utils.compute_arrival_departure(istdaten_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the arrival delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays = delays.withColumn('time_category', data_utils.time_cat(F.hour(F.from_unixtime('scheduled_arrival_time'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the transport_group and time_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_col = delays.withColumn('fullgroup', F.concat_ws(',', F.col('transport_group'), F.col('time_category')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "istdaten_sampled_full = data_utils.strat_sample(merged_col, 'fullgroup', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "istdaten_sampled_full.write.save(\"/user/boesinge/finalproject/istdaten_sampled.parquet\",mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
