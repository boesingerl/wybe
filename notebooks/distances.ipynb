{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to create 3 datasets out of the dataset 'geostops_csv' containing stop ids of stops located in a  15km,20km, and 30km radius from the Zurich train station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import geopandas\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "from geopy.distance import distance\n",
    "from io import StringIO\n",
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from ipywidgets import interactive, widgets, interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "# create connection\n",
    "conn = hive.connect(host=os.environ['HIVE_SERVER_2'], \n",
    "                    port=10000,\n",
    "                    username=username) \n",
    "# create cursor\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stops dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "query = \"\"\"\n",
    "    drop table if exists {0}.geostops_csv\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# create external table to read geostops data\n",
    "query = \"\"\"\n",
    "    create external table {0}.geostops_csv(\n",
    "        stop_id string,\n",
    "        stop_name string,\n",
    "        stop_lat string,\n",
    "        stop_lon string,\n",
    "        location_type string,\n",
    "        parent_station string\n",
    "    )\n",
    "    row format delimited fields terminated by ';'\n",
    "    stored as textfile\n",
    "    location '/data/sbb/csv/geostops/'\n",
    "    tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# select all here since it's a small database\n",
    "\n",
    "query = \"\"\"\n",
    "    select * from {0}.geostops_csv\n",
    "\"\"\".format(username)\n",
    "\n",
    "geostops_df = pd.read_sql(query, conn)\n",
    "geostops_df.columns = [x.split('.')[1] for x in geostops_df.columns]\n",
    "geostops_df[['stop_lat', 'stop_lon']] = geostops_df[['stop_lat', 'stop_lon']].astype('float')\n",
    "geostops_df['stop_id'] = geostops_df['stop_id'].apply(lambda x : x.split(':')[0].replace('P', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute dfs at a distance from Zurich HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# define lat/lon pair for zurich\n",
    "zurich_hb = (47.378177, 8.540192)\n",
    "\n",
    "# compute distance with respect to zurich_hb for each station\n",
    "distance_to_zurich = lambda x : distance((x['stop_lat'],x['stop_lon']), zurich_hb).km\n",
    "geostops_df['distance_to_main'] = geostops_df.apply(distance_to_zurich, axis=1)\n",
    "\n",
    "geostops_df = geostops_df.drop_duplicates(subset=['stop_id'])\n",
    "\n",
    "# get df and list of names of stations that are considered for start / end points\n",
    "df_15km = geostops_df[geostops_df['distance_to_main'] < 15].copy()\n",
    "df_20km = geostops_df[geostops_df['distance_to_main'] < 20].copy()\n",
    "df_30km = geostops_df[geostops_df['distance_to_main'] < 30].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize as string and pass to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "s_15km = StringIO()\n",
    "df_15km.to_csv(s_15km,encoding='utf-8')\n",
    "df_15km_val = s_15km.getvalue()\n",
    "\n",
    "s_20km = StringIO()\n",
    "df_20km.to_csv(s_20km,encoding='utf-8')\n",
    "df_20km_val = s_20km.getvalue()\n",
    "\n",
    "s_30km = StringIO()\n",
    "df_30km.to_csv(s_30km,encoding='utf-8')\n",
    "df_30km_val = s_30km.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'df_15km_val' as 'df_15km_val' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i df_15km_val -t str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'df_20km_val' as 'df_20km_val' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i df_20km_val -t str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'df_30km_val' as 'df_30km_val' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i df_30km_val -t str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.compat import StringIO\n",
    "\n",
    "df_15km = pd.read_csv(StringIO(df_15km_val),encoding='utf-8')\n",
    "df_20km = pd.read_csv(StringIO(df_20km_val),encoding='utf-8')\n",
    "df_30km = pd.read_csv(StringIO(df_30km_val),encoding='utf-8')\n",
    "\n",
    "df_15km = df_15km[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
    "df_20km = df_20km[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
    "df_30km = df_30km[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
    "\n",
    "sparkdf_15km = sqlContext.createDataFrame(df_15km)\n",
    "sparkdf_20km = sqlContext.createDataFrame(df_20km)\n",
    "sparkdf_30km = sqlContext.createDataFrame(df_30km)\n",
    "\n",
    "sparkdf_15km.write.save(\"/user/boesinge/finalproject/stops_15km.parquet\")\n",
    "sparkdf_20km.write.save(\"/user/boesinge/finalproject/stops_20km.parquet\")\n",
    "sparkdf_30km.write.save(\"/user/boesinge/finalproject/stops_30km.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
